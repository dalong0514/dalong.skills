<!-- 此文件由机器翻译自 theory.md -->

# SHAP理论基础

本文档解释了 SHAP（SHapley Additive exPlanations）的理论基础，包括博弈论中的 Shapley 值、SHAP 独特的原理以及与其他解释方法的联系。

## 博弈论起源

### 沙普利值

SHAP 基于 **Shapley 值**，这是劳埃德·沙普利 (Lloyd Shapley) 于 1951 年提出的合作博弈论的解决方案概念。

**核心理念**：
在合作博弈论中，玩家通过合作来获得总收益，问题是：这种收益应该如何在玩家之间公平分配？

**映射到机器学习**：
- **玩家** → 输入功能
- **游戏** → 模型预测任务
- **收益** → 模型输出（预测值）
- **联盟** → 具有已知值的特征子集
- **公平分配** → 将预测归因于特征

### Shapley 值公式

对于特征 $i$，其 Shapley 值 $\phi_i$ 为：

$$\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!} [f(S \cup \{i\}) - f(S)]$$

其中：
- $F$ 是所有特征的集合
- $S$ 是功能的子集，不包括 $i$
- $f(S)$ 是模型的预期输出，仅给出 $S$ 中的特征
- $|S|$ 是子集 $S$ 的大小

**解释**：
Shapley 值对所有可能的特征联盟（子集）中特征 $i$ 的边际贡献进行平均。贡献是根据每个联盟发生的可能性来加权的。

### Shapley 值的关键属性

**1.效率（可加性）**：
$$\sum_{i=1}^{n} \phi_i = f(x) - f(\emptyset)$$

所有 SHAP 值的总和等于模型对实例的预测与预期值（基线）之间的差。

这就是为什么 SHAP 瀑布图总和总是等于总预测变化的原因。

**2.对称性**：
如果两个特征 $i$ 和 $j$ 对所有联盟的贡献相等，则 $\phi_i = \phi_j$。

具有相同效果的功能会获得相同的归因。

**3.虚拟**：
如果特征 $i$ 不会改变任何联盟的模型输出，则 $\phi_i = 0$。

不相关的功能得到零归因。

**4.单调性**：
如果某个特征的边际贡献在联盟中增加，则其沙普利值也会增加。

## 从博弈论到机器学习

### 挑战

计算精确的 Shapley 值需要在所有可能的特征联盟上评估模型：
- 对于$n$个特征，有$2^n$个可能的联盟
- 对于 50 个功能，这超过 1 万亿次评估

这种指数级的复杂性使得精确计算对于大多数现实世界的模型来说都是困难的。

### SHAP 的解决方案：附加特征归因

SHAP 将 Shapley 值连接到**附加特征归因方法**，从而实现高效计算。

**附加特征归因模型**：
$$g(z') = \phi_0 + \sum_{i=1}^{M} \phi_i z'_i$$

其中：
- $g$ 是解释模型
- $z' \in \{0,1\}^M$ 表示特征存在/不存在
- $\phi_i$ 是特征 $i$ 的归属
- $\phi_0$ 是基线（期望值）

SHAP 证明 **Shapley 值是唯一满足三个理想属性**的属性值：局部准确性、缺失和一致性。

## SHA 属性和保证

### 本地准确度

**属性**：解释与模型的输出匹配：
$$f(x) = g(x') = \phi_0 + \sum_{i=1}^{M} \phi_i x'_i$$

**解释**：SHAP 值准确地解释了模型的预测。这使得瀑布图能够精确分解预测。

### 失踪

**属性**：如果某个功能缺失（未观察到），则其属性为零：
$$x'_i = 0 \右箭头 \phi_i = 0$$

**解释**：只有存在的功能才有助于解释。

### 一致性

**属性**：如果模型发生变化，导致某个特征对所有输入的边际贡献增加（或保持不变），则该特征的归因不应减少。

**解释**：如果某个特征对模型变得更加重要，它的 SHAP 值就会反映这一点。这使得有意义的模型比较成为可能。

## SHAP 作为统一框架

SHAP 统一了几种现有的解释方法，表明它们是特定假设下 Shapley 值的特殊情况。

### LIME（本地可解释的模型不可知的解释）

**LIME 的方法**：使用扰动样本围绕预测拟合局部线性模型。

**与 SHAP 的连接**：LIME 近似于 Shapley 值，但样本权重不是最优的。 SHAP 使用从 Shapley 值公式导出的理论上最佳权重。
**主要区别**：LIME 的损失函数和采样不能保证一致性或精确的可加性；夏普确实如此。

### 深度提升

**DeepLIFT 的方法**：通过与参考激活进行比较，通过神经网络反向传播贡献。

**与 SHAP 的连接**：DeepExplainer 使用 DeepLIFT，但对多个参考样本进行平均以近似条件期望，从而产生 Shapley 值。

### 逐层相关性传播 (LRP)

**LRP 的方法**：通过层向后传播相关性分数来分解神经网络预测。

**与 SHAP 的连接**：LRP 是具有特定传播规则的 SHAP 的特例。 SHAP 用 Shapley 值理论概括了这些规则。

### 积分梯度

**积分梯度方法**：沿着从基线到输入的路径积分梯度。

**连接到 SHAP**：使用单个参考点时，积分梯度会近似平滑模型的 SHAP 值。

## SHA 计算方法

不同的 SHAP 解释器使用专门的算法来有效计算特定模型类型的 Shapley 值。

### 树形 (TreeExplainer)

**创新**：利用树结构以多项式时间而不是指数时间计算精确的 Shapley 值。

**算法**：
- 从根到叶遍历每条树路径
- 使用树分割和权重计算特征贡献
- 集合中所有树的聚合

**复杂度**：$O(TLD^2)$，其中$T$ = 树的数量，$L$ = 最大叶子数，$D$ = 最大深度

**主要优势**：针对基于树的模型（XGBoost、LightGBM、随机森林等）有效计算精确的 Shapley 值

### 内核 SHAP（KernelExplainer）

**创新**：使用加权线性回归来估计任何模型的 Shapley 值。

**算法**：
- 根据 Shapley 核权重采样联盟（特征子集）
- 评估每个联盟的模型（缺失的特征由背景值替换）
- 拟合加权线性模型来估计特征属性

**复杂度**：$O(n \cdot 2^M)$，但使用较少的样本进行近似

**主要优势**：与模型无关；适用于任何预测函数

**权衡**：比专门的解释器慢；近似而非精确

### 深度 SHAP (DeepExplainer)

**创新**：将 DeepLIFT 与 Shapley 值采样相结合。

**算法**：
- 计算每个参考样本的 DeepLIFT 归因
- 多个参考样本的平均归因
- 近似条件期望：$E[f(x) | x_S]$

**复杂度**：$O(n \cdot m)$ 其中 $m$ = 参考样本数

**主要优势**：有效逼近深度神经网络的 Shapley 值

### 线性 SHAP (LinearExplainer)

**创新**：线性模型的闭合形式 Shapley 值。

**算法**：
- 对于独立特征：$\phi_i = w_i \cdot (x_i - E[x_i])$
- 对于相关特征：调整特征协方差

**复杂性**：$O(n)$ - 几乎是瞬时的

**主要优势**：以最少的计算获得精确的 Shapley 值

## 理解条件期望

### 核心挑战

计算 $f(S)$ （仅给出 $S$ 中的特征的模型输出）需要处理缺失的特征。

**问题**：当模型需要所有特征作为输入时，我们应该如何表示“缺失”的特征？

### 两种方法

**1.介入（边际）方法**：
- 用背景数据集中的值替换缺失的特征
- 估计：$E[f(x) | x_S]$ 通过边缘化 $x_{\bar{S}}$
- 解释：“如果我们不知道特征$\bar{S}$，模型会预测什么？”

**2.观察（有条件）方法**：
- 使用条件分布：$E[f(x) | x_S = x_S^*]$
- 考虑功能依赖性
- 解释：“对于具有 $S = x_S^*$ 特征的相似实例，模型会预测什么？”

**权衡**：
- **干预性**：更简单，假设特征独立，符合因果解释
- **观察**：相关特征更准确，需要条件分布估计

**TreeExplainer** 通过 `feature_perturbation` 参数支持两者。

## 基线（期望值）选择

**基线** $\phi_0 = E[f(x)]$ 表示模型的平均预测。

### 计算基线

**对于树解释器**：
- 使用背景数据：对背景数据集的平均预测
- 使用tree_path_dependent：使用树叶分布的加权平均值

**对于 DeepExplainer / KernelExplainer**：
- 背景样本的平均预测

### 基线的重要性
- SHAP 值测量与基线的偏差
- 不同的基线→不同的SHAP值（但仍然正确求和）
- 选择代表“典型”或“中性”输入的基线
- 常见选择：训练集均值、中位数或众数

## 解释 SHAP 值

### 单位和规模

**SHAP 值与模型输出具有相同的单位**：
- 回归：与目标变量相同的单位（美元、温度等）
- 分类（对数赔率）：对数赔率单位
- 分类（概率）：概率单位（如果模型输出已转换）

**幅度**：绝对 SHA 值越高 = 特征影响越强

**签名**：
- 正 SHA 值 = 特征将预测推高
- 负 SHAP 值 = 特征将预测推低

### 加法分解

对于预测 $f(x)$：
$$f(x) = E[f(X)] + \sum_{i=1}^{n} \phi_i(x)$$

**示例**：
- 预期值（基线）：0.3
- SHAP 值：{年龄：+0.15，收入：+0.10，教育程度：-0.05}
- 预测：0.3 美元 + 0.15 + 0.10 - 0.05 = 0.50 美元

### 全球重要性与本地重要性

**本地（实例级）**：
- 单次预测的 SHAP 值：$\phi_i(x)$
- 解释：“为什么模型在此实例中预测 $f(x)$？”
- 可视化：瀑布图、力图

**全局（数据集级别）**：
- 平均绝对 SHAP 值：$E[|\phi_i(x)|]$
- 解释：“总体而言哪些功能最重要？”
- 可视化：蜂群、条形图

**关键见解**：全局重要性是局部重要性的聚合，保持实例和数据集解释之间的一致性。

## SHAP 与其他特征重要性方法

### 与排列重要性的比较

**排列重要性**：
- 随机排列一个特征并测量准确度下降
- 仅全局指标（无实例级解释）
- 相关特征可能会产生误导

**形状**：
- 提供本地和全球重要性
- 通过联合平均处理特征相关性
- 一致：可加性保证了预测的总和

### 与特征系数的比较（线性模型）

**特征系数** ($w_i$)：
- 衡量每单位功能变化的影响
- 不考虑特征规模或分布

**线性模型的SHAP**：
- $\phi_i = w_i \cdot (x_i - E[x_i])$
- 考虑相对于平均值的特征值
- 更容易解释不同单位/尺度的特征比较

### 与树特征重要性的比较（基于基尼/分割）

**基尼/分割重要性**：
- 基于训练过程（纯度增益或分裂频率）
- 偏向于高基数特征
- 没有实例级解释
- 可能会产生误导（重要性≠预测能力）

**形状（树形）**：
- 基于模型输出（预测行为）
- 通过 Shapley 值进行公平归因
- 提供实例级解释
- 一致且有理论依据

## 交互作用和高阶效应

### SHAP 交互值

标准 SHAP 捕捉主效应。 **SHAP 交互值** 捕获成对交互。

**交互公式**：
$$\phi_{i,j} = \sum_{S \subseteq F \setminus \{i,j\}} \frac{|S|!(|F|-|S|-2)!}{2(|F|-1)!} \Delta_{ij}(S)$$

其中 $\Delta_{ij}(S)$ 是给定联盟 $S$ 的特征 $i$ 和 $j$ 的交互效果。

**解释**：
- $\phi_{i,i}$：特征$i$的主要效果
- $\phi_{i,j}$ ($i \neq j$): 特征$i$和$j$之间的交互效果

**财产**：
$$\phi_i = \phi_{i,i} + \sum_{j \neq i} \phi_{i,j}$$

主 SHAP 值等于主效应加上涉及特征 $i$ 的所有成对相互作用的一半。

### 计算交互

**TreeExplainer** 支持精确的交互计算：
```python
explainer = shap.TreeExplainer(model)
shap_interaction_values = explainer.shap_interaction_values(X)
```

**限制**：对于其他解释器来说指数复杂（仅适用于树模型）

## 理论限制和注意事项

### 计算复杂性

**精确计算**：$O(2^n)$ - 对于大的$n$来说是棘手的

**专业算法**：
- 树形状：$O(TLD^2)$ - 对树有效
- 深度 SHAP、内核 SHAP：需要近似值

**含义**：对于具有许多特征的非树模型，解释可能是近似的。

### 特征独立性假设

**内核 SHAP 和基本实现**：假设功能可以独立操作

**挑战**：真实特征通常是相关的（例如身高和体重）

**解决方案**：
- 使用观察方法（条件期望）
- 具有相关感知扰动的 TreeExplainer
- 高度相关特征的特征分组

### 分布外样本
**问题**：通过替换特征创建联盟可能会创建不切实际的样本（训练分布之外）

**示例**：同时设置“年龄=5”和“拥有博士学位=是”

**含义**：SHAP 值反映了模型对潜在不切实际的输入的行为

**缓解**：使用观察方法或精心选择的背景数据

### 因果关系

**SHAP 衡量的是关联性，而不是因果性**

SHAP 回答：“模型的预测如何随此功能而改变？”
SHAP 没有回答：“如果我们在现实中改变这个功能会发生什么？”

**示例**：
- SHAP：“住院时间增加了死亡率的预测”（协会）
- 因果关系：“住院时间越长，死亡率越高”（不正确！）

**含义**：利用领域知识因果地解释SHAP； SHAP 本身并不能确定因果关系。

## 高级理论主题

### SHAP 作为最优信用分配

SHAP 是满足以下条件的独特归因方法：
1. **局部精度**：解释与模型匹配
2. **缺失**：缺失的特征具有零归因
3. **一致性**：归因反映了特征重要性的变化

**证明**：Lundberg & Lee (2017) 表明 Shapley 值是满足这些公理的唯一解决方案。

### 与函数方差分析的联系

SHAP 值对应于函数 ANOVA 分解中的一阶项：
$$f(x) = f_0 + \sum_i f_i(x_i) + \sum_{i,j} f_{ij}(x_i, x_j) + ...$$

其中 $f_i(x_i)$ 捕获特征 $i$ 的主效应，$\phi_i \approx f_i(x_i)$。

### 与敏感性分析的关系

SHAP 概括了敏感性分析：
- **敏感性分析**：$\frac{\partial f}{\partial x_i}$（局部梯度）
- **SHAP**：特征联盟空间的综合敏感性

基于梯度的方法（GradientExplainer、Integrated Gradients）使用导数来近似 SHAP。

## 理论的实际意义

### 为什么使用 SHAP？

1. **理论保证**：唯一具有一致性、局部准确性和缺失的方法
2. **统一框架**：连接并概括多种解释方法
3. **加法分解**：预测精确分解为特征贡献
4. **模型比较**：一致性可以比较模型之间的特征重要性
5. **多功能性**：适用于任何模型类型（带有适当的解释器）

### 何时要谨慎

1. **计算成本**：对于没有专门解释器的复杂模型来说可能会很慢
2. **特征相关性**：标准方法可能会创建不切实际的样本
3. **解释**：需要了解基线、单位和假设
4. **因果关系**：SHAP并不意味着因果关系；使用领域知识
5. **近似值**：非树方法使用近似值；了解准确性权衡

## 参考文献和进一步阅读

**基础论文**：
- 沙普利，L.S. (1951)。 “n人游戏的价值”
- Lundberg, S.M. 和 Lee, S.I. (2017)。 “解释模型预测的统一方法”（NeurIPS）
- Lundberg, S. M. 等人（2020）。 “通过可解释的树木人工智能从本地解释到全球理解”（自然机器智能）

**关键概念**：
- 合作博弈论和沙普利值
- 附加特征归因方法
- 条件期望估计
- 树SHAP算法和多项式时间计算

这一理论基础解释了为什么 SHAP 是一种有原则的、多功能的、强大的模型解释工具。