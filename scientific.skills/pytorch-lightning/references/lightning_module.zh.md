<!-- 此文件由机器翻译自 lightning_module.md -->

# LightningModule - 综合指南

## 概述

`LightningModule` 将 PyTorch 代码组织成六个没有抽象的逻辑部分。代码仍然是纯粹的 PyTorch，只是组织得更好。 Trainer 处理设备管理、分布式采样和基础设施，同时保留完整的模型控制。

## 核心结构

```python
import lightning as L
import torch
import torch.nn.functional as F

class MyModel(L.LightningModule):
    def __init__(self, learning_rate=0.001):
        super().__init__()
        self.save_hyperparameters()  # Save init arguments
        self.model = YourNeuralNetwork()

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self.model(x)
        loss = F.cross_entropy(logits, y)
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self.model(x)
        loss = F.cross_entropy(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()
        self.log("val_loss", loss)
        self.log("val_acc", acc)

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self.model(x)
        loss = F.cross_entropy(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()
        self.log("test_loss", loss)
        self.log("test_acc", acc)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "val_loss"
            }
        }
```

## 基本方法

### 训练管道方法

#### `training_step(batch, batch_idx)`
计算前向传递并返回损失。 Lightning 在自动优化模式下自动处理反向传播和优化器更新。

**参数：**
- `batch` - 来自 DataLoader 的当前训练批次
- `batch_idx` - 当前批次的索引

**返回：** 损失张量（标量）或带有“loss”键的字典

**示例：**
<<<代码块_1>>>

#### `validation_step(batch, batch_idx)`
根据验证数据评估模型。禁用渐变并自动在评估模式下运行模型。

**参数：**
- `batch` - 当前验证批次
- `batch_idx` - 当前批次的索引

**返回：**可选 - 损失或指标字典

**示例：**
<<<代码块_2>>>

#### `test_step(batch, batch_idx)`
根据测试数据评估模型。仅在使用 `trainer.test()` 显式调用时运行。培训完成后使用，通常在发布前。

**参数：**
- `batch` - 当前测试批次
- `batch_idx` - 当前批次的索引

**返回：**可选 - 损失或指标字典

#### `predict_step(batch, batch_idx, dataloader_idx=0)`
对数据运行推理。使用 `trainer.predict()` 时调用。

**参数：**
- `batch` - 当前批次
- `batch_idx` - 当前批次的索引
- `dataloader_idx` - 数据加载器索引（如果有多个）

**返回：** 预测（您需要的任何格式）

**示例：**
<<<代码块_3>>>

### 配置方法

#### `configure_optimizers()`
返回优化器和可选的学习率调度器。

**返回格式：**

1. **单一优化器：**
<<<代码块_4>>>

2. **优化器+调度器：**
<<<代码块_5>>>

3. **具有调度程序监控的高级配置：**
<<<代码块_6>>>

4. **多个优化器（针对 GAN 等）：**
```python
def configure_optimizers(self):
    opt_g = torch.optim.Adam(self.generator.parameters(), lr=0.0002)
    opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)
    return [opt_g, opt_d]
```

#### `forward(*args, **kwargs)`
标准 PyTorch 前向方法。用于推理或作为训练步骤的一部分。

**示例：**
```python
def forward(self, x):
    return self.model(x)

def training_step(self, batch, batch_idx):
    x, y = batch
    y_hat = self(x)  # Uses forward()
    return F.mse_loss(y_hat, y)
```

### 日志记录和指标

#### `log(name, value, **kwargs)`
跨设备自动减少纪元级别记录指标。

**关键参数：**
- `name` - 指标名称（字符串）
- `value` - 度量值（张量或数字）
- `on_step` - 在当前步骤记录（默认值：training_step 中为 True，否则为 False）
- `on_epoch` - 在纪元结束时记录（默认值：training_step 中为 False，否则为 True）
- `prog_bar` - 在进度条中显示（默认值：False）
- `logger` - 发送到记录器后端（默认值：True）
- `reduce_fx` - 缩减函数：“mean”、“sum”、“max”、“min”（默认值：“mean”）
- `sync_dist` - 在分布式训练中跨设备同步（默认值：False）

**示例：**
```python
# Simple logging
self.log("train_loss", loss)

# Display in progress bar
self.log("accuracy", acc, prog_bar=True)

# Log per-step and per-epoch
self.log("loss", loss, on_step=True, on_epoch=True)

# Custom reduction for distributed training
self.log("batch_size", batch.size(0), reduce_fx="sum", sync_dist=True)
```

#### `log_dict(dictionary, **kwargs)`
同时记录多个指标。

**示例：**
```python
metrics = {"train_loss": loss, "train_acc": acc, "learning_rate": lr}
self.log_dict(metrics, on_step=True, on_epoch=True)
```

#### `save_hyperparameters(*args, **kwargs)`
存储用于再现性和检查点恢复的初始化参数。调用`__init__()`。

**示例：**
```python
def __init__(self, learning_rate, hidden_dim, dropout):
    super().__init__()
    self.save_hyperparameters()  # Saves all init args
    # Access via self.hparams.learning_rate, self.hparams.hidden_dim, etc.
```

## 关键属性

|物业 |描述 |
|----------|-------------|
| `self.current_epoch` |当前纪元号（0 索引）|
| `self.global_step` |跨所有时期的总优化器步骤 |
| `self.device` |当前设备（cuda:0、cpu 等）|
| `self.global_rank` |分布式训练中的进程排名（0 表示主要）|
| `self.local_rank` |当前节点上的 GPU 排名 |
| `self.hparams` |保存的超参数（通过 save_hyperparameters）|
| `self.trainer` |引用父 Trainer 实例 |
| `self.automatic_optimization` |是否使用自动优化（默认：True）|

## 手动优化

对于高级用例（GAN、强化学习、多个优化器），禁用自动优化：

```python
class GANModel(L.LightningModule):
    def __init__(self):
        super().__init__()
        self.automatic_optimization = False
        self.generator = Generator()
        self.discriminator = Discriminator()

    def training_step(self, batch, batch_idx):
        opt_g, opt_d = self.optimizers()

        # Train generator
        opt_g.zero_grad()
        g_loss = self.compute_generator_loss(batch)
        self.manual_backward(g_loss)
        opt_g.step()

        # Train discriminator
        opt_d.zero_grad()
        d_loss = self.compute_discriminator_loss(batch)
        self.manual_backward(d_loss)
        opt_d.step()

        self.log_dict({"g_loss": g_loss, "d_loss": d_loss})

    def configure_optimizers(self):
        opt_g = torch.optim.Adam(self.generator.parameters(), lr=0.0002)
        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)
        return [opt_g, opt_d]
```

## 重要的生命周期挂钩

### 设置和拆卸

#### `setup(stage)`
在拟合、验证、测试或预测开始时调用。对于特定于阶段的设置很有用。

**参数：**
- `stage` - '拟合'、'验证'、'测试'或'预测'

**示例：**
```python
def setup(self, stage):
    if stage == 'fit':
        # Setup training-specific components
        self.train_dataset = load_train_data()
    elif stage == 'test':
        # Setup test-specific components
        self.test_dataset = load_test_data()
```

#### `teardown(stage)`
在拟合、验证、测试或预测结束时调用。清理资源。

### 纪元边界

#### `on_train_epoch_start()` / `on_train_epoch_end()`
在每个训练周期的开始/结束时调用。

**示例：**
```python
def on_train_epoch_end(self):
    # Compute epoch-level metrics
    all_preds = torch.cat(self.training_step_outputs)
    epoch_metric = compute_custom_metric(all_preds)
    self.log("epoch_metric", epoch_metric)
    self.training_step_outputs.clear()  # Free memory
```

#### `on_validation_epoch_start()` / `on_validation_epoch_end()`
在验证时期的开始/结束时调用。

#### `on_test_epoch_start()` / `on_test_epoch_end()`
在测试纪元的开始/结束时调用。

### 渐变挂钩

#### `on_before_backward(loss)`
在loss.backward()之前调用。

#### `on_after_backward()`
在 loss.backward() 之后但在优化器步骤之前调用。

**示例 - 梯度检查：**
```python
def on_after_backward(self):
    # Log gradient norms
    grad_norm = torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
    self.log("grad_norm", grad_norm)
```

### 检查点挂钩

#### `on_save_checkpoint(checkpoint)`
自定义检查点保存。添加额外的状态来保存。

**示例：**
```python
def on_save_checkpoint(self, checkpoint):
    checkpoint['custom_state'] = self.custom_data
```

#### `on_load_checkpoint(checkpoint)`
自定义检查点加载。恢复额外状态。

**示例：**
```python
def on_load_checkpoint(self, checkpoint):
    self.custom_data = checkpoint.get('custom_state', default_value)
```

## 最佳实践

### 1. 设备不可知论
切勿使用显式 `.cuda()` 或 `.cpu()` 调用。 Lightning 会自动处理设备放置。

**不好：**
```python
x = x.cuda()
model = model.cuda()
```

**好：**
```python
x = x.to(self.device)  # Inside LightningModule
# Or let Lightning handle it automatically
```

### 2.分布式训练安全
不要手动创建`DistributedSampler`。闪电会自动处理这个问题。

**不好：**
```python
sampler = DistributedSampler(dataset)
DataLoader(dataset, sampler=sampler)
```

**好：**
```python
DataLoader(dataset, shuffle=True)  # Lightning converts to DistributedSampler
```

### 3. 指标聚合
使用 `self.log()` 进行自动跨设备缩减，而不是手动收集。

**不好：**
```python
self.validation_outputs.append(loss)

def on_validation_epoch_end(self):
    avg_loss = torch.stack(self.validation_outputs).mean()
```

**好：**
```python
self.log("val_loss", loss)  # Automatic aggregation
```

### 4. 超参数跟踪
始终使用 `self.save_hyperparameters()` 来轻松重新加载模型。

**示例：**
```python
def __init__(self, learning_rate, hidden_dim):
    super().__init__()
    self.save_hyperparameters()

# Later: Load from checkpoint
model = MyModel.load_from_checkpoint("checkpoint.ckpt")
print(model.hparams.learning_rate)
```

### 5. 验证放置
在单个设备上运行验证，以确保每个样本仅评估一次。闪电网络会通过正确的策略配置自动处理此问题。

## 从检查点加载

```python
# Load model with saved hyperparameters
model = MyModel.load_from_checkpoint("path/to/checkpoint.ckpt")

# Override hyperparameters if needed
model = MyModel.load_from_checkpoint(
    "path/to/checkpoint.ckpt",
    learning_rate=0.0001  # Override saved value
)

# Use for inference
model.eval()
predictions = model(data)
```

## 常见模式

### 梯度累积
让 Lightning 处理梯度累积：

```python
trainer = L.Trainer(accumulate_grad_batches=4)
```

### 渐变裁剪
在训练器中配置：

```python
trainer = L.Trainer(gradient_clip_val=0.5, gradient_clip_algorithm="norm")
```

### 混合精度训练
在 Trainer 中配置精度：

```python
trainer = L.Trainer(precision="16-mixed")  # or "bf16-mixed", "32-true"
```

### 学习率预热
在configure_optimizers中实现：

```python
def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
    scheduler = {
        "scheduler": torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=0.01,
            total_steps=self.trainer.estimated_stepping_batches
        ),
        "interval": "step"
    }
    return [optimizer], [scheduler]
```