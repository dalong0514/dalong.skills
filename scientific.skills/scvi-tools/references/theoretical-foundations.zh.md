<!-- 此文件由机器翻译自 theoretical-foundations.md -->

# scvi-tools 的理论基础

本文档解释了 scvi 工具的数学和统计原理。

## 核心概念

### 变分推理

**这是什么？**
变分推理是一种近似复杂概率分布的技术。在单细胞分析中，我们想要了解后验分布 p(z|x) - 给定观测数据 x 的潜在变量 z 的概率。

**为什么要使用它？**
- 对于复杂模型来说，精确推理在计算上是困难的
- 扩展到大型数据集（数百万个单元格）
- 提供不确定性量化
- 启用关于细胞状态的贝叶斯推理

**它是如何工作的？**
1. 定义一个具有可学习参数的更简单的近似分布 q(z|x)
2. 最小化 q(z|x) 和真实后验 p(z|x) 之间的 KL 散度
3. 相当于最大化证据下界（ELBO）

**ELBO 目标**：
```
ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z))
       ↑                    ↑
  Reconstruction          Regularization
```

- **重建项**：模型应生成与观察到的数据类似的数据
- **正则化项**：潜在表示应与先前匹配

### 变分自动编码器 (VAE)

**架构**：
<<<代码块_1>>>

**编码器**：将单元格 (x) 映射到潜在空间 (z)
- 学习 q(z|x)，近似后验概率
- 通过具有可学习权重的神经网络进行参数化
- 输出潜在分布的均值和方差

**解码器**：将潜在空间（z）映射回基因空间
- 学习 p(x|z)，即可能性
- 从潜在表征生成基因表达
- 模型计数分布（负二项式、零膨胀 NB）

**重新参数化技巧**：
- 允许通过随机采样进行反向传播
- 样本 z = μ + σ ⊙ ε，其中 ε ~ N(0,1)
- 通过梯度下降实现端到端训练

### 摊销推理

**概念**：在所有单元之间共享编码器参数。

**传统推理**：学习每个单元格的单独潜在变量
- n_cells × n_latent 参数
- 无法扩展到大型数据集

**摊销推理**：学习所有单元格的单个编码器
- 固定数量的参数，与细胞计数无关
- 能够对新单元进行快速推理
- 跨数据集转移学习的模式

**好处**：
- 可扩展到数百万个单元
- 对查询数据进行快速推理
- 利用跨单元的共享结构
- 实现少量学习

## 统计建模

### 统计数据分布

单细胞数据是计数（整数值），需要适当的分布。

#### 负二项式 (NB)
<<<代码块_2>>>
- **μ（平均值）**：预期表达水平
- **θ（色散）**：控制方差
- **方差**：Var(x) = μ + μ²/θ

**何时使用**：无零膨胀的基因表达
- 比泊松更灵活（允许过度分散）
- 建立技术和生物变异模型

#### 零膨胀负二项式 (ZINB)
<<<代码块_3>>>
- **π（辍学率）**：技术零的概率
- **δ₀**：点质量为零
- **NB(μ, θ)**：未退出时的表达式

**何时使用**：稀疏 scRNA-seq 数据
- 分别对技术零点和生物零点进行建模
- 更适合高度稀疏的数据（例如 10 倍数据）

#### 泊松
<<<代码块_4>>>
- 最简单的计数分布
- 均值等于方差：Var(x) = μ

**何时使用**：不太常见； ATAC-seq 片段计数
- 比 NB 更严格
- 更快的计算

### 批量修正框架

**问题**：技术变化混淆了生物信号
- 不同的测序运行、方案、实验室
- 必须消除技术影响，同时保护生物学

**scvi 工具方法**：
1. 将batch编码为分类变量s
2. 在生成模型中包含 s
3. 潜在空间 z 是批次不变的
4. s 上针对批次特定效果的解码器条件

**数学公式**：
<<<代码块_5>>>

**关键见解**：批量信息流经解码器，而不是潜在空间
- z 捕获生物变异
-s 解释技术变化
- 可分离的生物学效应和批次效应

### 深度生成建模

**生成模型**：学习数据分布 p(x)

**流程**：
1. 样本潜变量：z ~ p(z) = N(0, I)
2. 生成表达式：x ~ p(x|z)
3.联合分布：p(x,z) = p(x|z)p(z)

**好处**：
- 生成合成细胞
- 估算缺失值
- 量化不确定性
- 执行反事实预测

**推理网络**：反转生成过程
- 给定 x，推断 z
- q(z|x) 近似真实后验 p(z|x)

## 模型架构细节

### scVI架构

**输入**：基因表达计数 x ∈ ℕ^G（G 基因）

**编码器**：
<<<代码块_6>>>

**潜在空间**：z ∈ ℝ^d（通常 d=10-30）

**解码器**：
```
h = ReLU(W₄·z + b₄)
μ = softmax(W₅·h + b₅) · library_size
θ = exp(W₆·h + b₆)
π = sigmoid(W₇·h + b₇)  # for ZINB
x ~ ZINB(μ, θ, π)
```

**损失函数（ELBO）**：
```
L = E_q[log p(x|z)] - KL(q(z|x) || N(0,I))
```

### 处理协变量

**分类协变量**（批次、捐赠者等）：
- One-hot 编码：s ∈ {0,1}^K
- 连接潜在：[z, s]
- 或者使用条件层

**连续协变量**（文库大小，percent_mito）：
- 标准化为零均值、单位方差
- 包含在编码器和/或解码器中

**协变量注入策略**：
- **串联**：[z, s] 馈送到解码器
- **深度注入**：多层添加
- **条件批量归一化**：特定于批次的归一化

## 高级理论概念

### 迁移学习 (scArches)

**概念**：使用预训练模型作为新数据的初始化

**流程**：
1. 在大数据集上训练参考模型
2. 冻结编码器参数
3. 根据查询数据微调解码器
4. 或者以较低的学习率进行全部微调

**为什么它有效**：
- 编码器学习一般细胞表示
- 解码器适应查询特定的特征
- 防止灾难性遗忘

**应用**：
- 查询到引用的映射
- 稀有细胞类型的少样本学习
- 新数据集的快速分析

### 多分辨率建模 (MrVI)

**想法**：单独的共享变体和特定于样本的变体

**潜在空间分解**：
```
z = z_shared + z_sample
```
- **z_shared**：跨样本通用
- **z_sample**：样本特定效果

**层次结构**：
```
Sample level: ρ_s ~ N(0, I)
Cell level: z_i ~ N(ρ_{s(i)}, σ²)
```

**好处**：
- 理清变异的生物来源
- 比较不同分辨率下的样本
- 识别样本特定的细胞状态

### 反事实预测

**目标**：预测不同条件下的结果

**示例**：“如果该细胞来自不同批次，它会是什么样子？”

**方法**：
1. 将单元格编码为潜在的： z = Encoder(x, s_original)
2. 用新条件解码：x_new = Decoder(z, s_new)
3. x_new是反事实预测

**应用**：
- 批量效果评估
- 预测治疗反应
- 计算机微扰研究

### 后验预测分布

**定义**：给定观测数据的新数据分布

```
p(x_new | x_observed) = ∫ p(x_new|z) q(z|x_observed) dz
```

**估计**：从 q(z|x) 中采样 z，从 p(x_new|z) 生成 x_new

**用途**：
- 不确定性量化
- 稳健的预测
- 异常值检测

## 差异表达框架

### 贝叶斯方法

**传统方法**：比较点估计
- Wilcoxon、t 检验等。
- 忽略不确定性
- 需要伪计数

**scvi-tools 方法**：比较发行版
- 后验样本：μ_A ~ p(μ|x_A), μ_B ~ p(μ|x_B)
- 计算对数倍数变化：LFC = log(μ_B) - log(μ_A)
- LFC 的后验分布量化了不确定性

### 贝叶斯因子

**定义**：后验赔率与先验赔率的比率

```
BF = P(H₁|data) / P(H₀|data)
     ─────────────────────────
     P(H₁) / P(H₀)
```

**解释**：
- BF > 3：H₁ 的中等证据
- BF > 10：强有力的证据
- BF > 100：决定性证据

**在 scvi-tools 中**：用于根据 DE 证据对基因进行排名

### 错误发现比例 (FDP)

**目标**：控制预期错误发现率

**程序**：
1. 对于每个基因，计算DE的后验概率
2. 按证据对基因进行排序（贝叶斯因子）
3. 选择前 k 个基因，使得 E[FDP] ≤ α

**相对于 p 值的优势**：
- 完全贝叶斯
- 自然的后验推理
- 没有任意阈值

## 实施细节

### 优化

**优化器**：Adam（自适应学习率）
- 默认 lr = 0.001
- 动量参数：β₁=0.9，β2=0.999

**训练循环**：
1. 小批量细胞样本
2.计算ELBO损失
3.反向传播梯度
4. 使用 Adam 更新参数
5. 重复直到收敛

**收敛标准**：
- 验证集上的 ELBO 平台
- 提前停止可防止过度拟合
- 通常为 200-500 epoch

### 正则化

**KL退火**：逐渐增加KL权重
- 防止后塌陷
- 从 0 开始，经过 epoch 增加到 1

**Dropout**：训练期间随机神经元丢失
- 默认值：0.1 辍学率
- 防止过度拟合
- 提高泛化能力

**权重衰减**：权重的 L2 正则化
- 防止大重量
- 提高稳定性

### 可扩展性

**小批量训练**：
- 每次迭代处理单元格子集
- 批量大小：64-256 个细胞
- 能够扩展到数百万个单元

**随机优化**：
- 估计小批量的 ELBO
- 无偏梯度估计
- 收敛到最优解

**GPU加速**：
- 神经网络自然并行
- 数量级的加速
- 对于大型数据集至关重要

## 与其他方法的连接

### 与 PCA
- **PCA**：线性、确定性
- **scVI**：非线性、概率
- **优点**：scVI 捕获复杂结构，处理计数

### 与 t-SNE/UMAP
- **t-SNE/UMAP**：注重可视化
- **scVI**：完整的生成模型
- **优点**：scVI 支持下游任务（DE、插补）

### 与 Seurat 整合
- **Seurat**：基于锚点的对齐
- **scVI**：概率建模
- **优点**：scVI 提供不确定性，适用于多个批次

### VS 和谐
- **Harmony**：PCA + 批量校正
- **scVI**：基于 VAE
- **优点**：scVI 原生处理计数，更灵活

## 数学符号

**常用符号**：
- x：观察到的基因表达（计数）
- z：潜在表示
- θ：模型参数
- q(z|x)：近似后验（编码器）
- p(x|z)：可能性（解码器）
- p(z)：潜在变量的先验
- μ、σ²：均值和方差
- π：辍学概率（ZINB）
- θ（NB）：色散参数
- s：批次/协变量指标

## 进一步阅读

**关键论文**：
1.洛佩兹等人。 （2018）：“单细胞转录组学的深度生成模型”
2.徐等人。 （2021）：“单细胞转录组学的概率协调和注释”
3.博约等人。 (2019)：“用于检测单细胞差异表达的深度生成模型”

**要探索的概念**：
- 机器学习中的变分推理
- 贝叶斯深度学习
- 信息论（KL散度、互信息）
- 生成模型（GAN、标准化流、扩散模型）
- 概率编程（Pyro、PyTorch）

**数学背景**：
- 概率论和统计学
- 线性代数和微积分
- 最优化理论
- 信息论