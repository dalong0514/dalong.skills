<!-- 此文件由机器翻译自 algorithms.md -->

# Stable Baselines3 算法参考

本文档提供了 Stable Baselines3 中所有 RL 算法的详细特征，以帮助为特定任务选择正确的算法。

## 算法对照表

|算法|类型 |行动空间|样品效率 |训练速度|使用案例|
|------------|------|--------------|--------------------|----------------|----------|
| **PPO** |在保单 |全部 |中等|快|通用、稳定 |
| **A2C** |在保单 |全部 |低|非常快|快速原型设计、多重处理 |
| **SAC** |政策外 |连续 |高|中等|连续控制，样品高效 |
| **TD3** |政策外 |连续 |高|中等|连续控制，确定性 |
| **DDPG** |政策外 |连续|高|中等|连续控制（使用TD3代替） |
| **DQN** |政策外 |离散|中等|中等|离散动作，Atari 游戏 |
| **她** |政策外 |全部 |非常高 |中等|目标条件任务 |
| **复发性 PPO** |在保单 |全部 |中等|慢|部分可观测性（POMDP）|

## 详细的算法特点

### PPO（邻近策略优化）

**概述：** 通用的同策略算法，在许多任务中具有良好的性能。

**优点：**
- 稳定可靠的训练
- 适用于所有动作空间类型（离散、盒子、多离散、多二进制）
- 样本效率和训练速度之间的良好平衡
- 非常适合矢量化环境的多处理
- 易于调整

**弱点：**
- 样本效率低于离策略方法
- 需要许多环境交互

**最适合：**
- 通用强化学习任务
- 当稳定性很重要时
- 当你有廉价的环境模拟时
- 具有连续或离散动作的任务

**超参数指导：**
- `n_steps`：连续为 2048-4096，Atari 为 128-256
- `learning_rate`：3e-4 是一个很好的默认值
- `n_epochs`：10 表示连续，4 表示 Atari
- `batch_size`：64
- `gamma`：0.99（长集为 0.995-0.999）

### A2C（优势演员评论家）

**概述：** A3C的同步变体，比PPO简单，但稳定性较差。

**优点：**
- 非常快速的训练（比 PPO 简单）
- 适用于所有动作空间类型
- 适合快速原型制作
- 内存高效

**弱点：**
- 稳定性不如 PPO
- 需要仔细调整超参数
- 样品效率较低

**最适合：**
- 快速实验
- 当训练速度至关重要时
- 简单的环境

**超参数指导：**
- `n_steps`：5-256，具体取决于任务
- `learning_rate`：7e-4
- `gamma`：0.99

### SAC（软演员评论家）

**概述：** 具有熵正则化的离策略算法，最先进的连续控制。

**优点：**
- 出色的样品效率
- 非常稳定的训练
- 自动熵调整
- 通过随机政策进行良好的探索
- 最先进的机器人技术

**弱点：**
- 仅支持连续动作空间（Box）
- 挂钟时间比 on-policy 方法慢
- 更复杂的超参数

**最适合：**
- 连续控制（机器人、物理模拟）
- 当样品效率至关重要时
- 昂贵的环境模拟
- 需要良好探索的任务

**超参数指导：**
- `learning_rate`：3e-4
- `buffer_size`：大多数任务为 1M
- `learning_starts`：10000
- `batch_size`：256
- `tau`：0.005（目标网络更新率）
- `train_freq`：1 与 `gradient_steps=-1` 以获得最佳性能

### TD3（双延迟 DDPG）

**概述：** 通过双 Q 学习和延迟策略更新改进了 DDPG。

**优点：**
- 样品效率高
- 确定性策略（有利于部署）
- 比DDPG更稳定
- 适合连续控制

**弱点：**
- 仅支持连续动作空间（Box）
- 比 SAC 更少的探索
- 需要仔细调整

**最适合：**
- 连续控制任务
- 当首选确定性策略时
- 样本高效学习

**超参数指导：**
- `learning_rate`：1e-3
- `buffer_size`：1M
- `learning_starts`：10000
- `batch_size`：100
- `policy_delay`：2（每 2 个评论家更新更新一次政策）

### DDPG（深度确定性策略梯度）

**概述：** 早期的离策略连续控制算法。

**优点：**
- 持续行动空间支持
- 离政策学习

**弱点：**
- 稳定性不如 TD3 或 SAC
- 对超参数敏感
- 总体表现优于 TD3

**最适合：**
- 旧版兼容性
- **建议：** 对于新项目使用 TD3

### DQN（深度 Q 网络）

**概述：** 用于离散动作空间的经典离策略算法。

**优点：**
- 离散动作的样本效率
- 经验回放可以重复使用过去的数据
- 在 Atari 游戏上取得的成功

**弱点：**
- 仅支持离散动作空间
- 如果没有适当的调整可能会不稳定
- 高估偏差

**最适合：**
- 离散行动任务
- Atari 游戏和类似环境
- 当样品效率很重要时

**超参数指导：**
- `learning_rate`：1e-4
- `buffer_size`：100K-1M，具体取决于任务
- `learning_starts`：雅达利 50000
- `batch_size`：32
- `exploration_fraction`：0.1
- `exploration_final_eps`：0.05

**变体：**
- **QR-DQN**：分布式强化学习版本，可实现更好的价值估计
- **可屏蔽 DQN**：适用于具有动作屏蔽的环境

### HER（事后经验重播）

**概述：** 不是一个独立的算法，而是针对目标条件任务的重播缓冲策略。

**优点：**
- 显着改善稀疏奖励环境中的学习
- 通过重新标记目标从失败中学习
- 适用于任何离策略算法（SAC、TD3、DQN）

**弱点：**
- 仅适用于目标条件环境
- 需要特定的观察结构（带有“观察”、“实现目标”、“期望目标”的字典）

**最适合：**
- 目标条件任务（机器人操作、导航）
- 稀疏的奖励环境
- 目标明确但奖励是二元的任务

**用途：**
```python
from stable_baselines3 import SAC, HerReplayBuffer

model = SAC(
    "MultiInputPolicy",
    env,
    replay_buffer_class=HerReplayBuffer,
    replay_buffer_kwargs=dict(
        n_sampled_goal=4,
        goal_selection_strategy="future",  # or "episode", "final"
    ),
)
```

### 复发性PPO

**概述：** PPO 具有 LSTM 策略，用于处理部分可观察性。

**优点：**
- 处理部分可观测性（POMDP）
- 可以学习时间依赖性
- 适合需要记忆的任务

**弱点：**
- 训练速度比标准 PPO 慢
- 调整更复杂
- 需要连续数据

**最适合：**
- 部分可观察的环境
- 需要记忆的任务（例如，没有完整地图的导航）
- 时间序列问题

## 算法选择指南

### 决策树

1. **你的行动空间是什么？**
   - **连续（方框）** → 考虑 PPO、SAC 或 TD3
   - **离散** → 考虑 PPO、A2C 或 DQN
   - **多离散/多二进制** → 使用 PPO 或 A2C

2. **样品效率至关重要吗？**
   - **是（昂贵的模拟）** → 使用离策略：SAC、TD3、DQN 或 HER
   - **否（廉价模拟）** → 使用 on-policy：PPO、A2C

3. **您需要快速挂钟训练吗？**
   - **是** → 在矢量化环境中使用 PPO 或 A2C
   - **否** → 任何算法都有效

4. **任务是否以稀疏奖励为目标条件？**
   - **是** → 将 HER 与 SAC 或 TD3 一起使用
   - **否** → 继续使用标准算法

5. **环境是否可以部分观察？**
   - **是** → 使用 RecurrentPPO
   - **否** → 使用标准算法

### 快速推荐

- **开始/一般任务：** PPO
- **连续控制/机器人：** SAC
- **离散操作/Atari：** DQN 或 PPO
- **目标条件/稀疏奖励：** SAC + HER
- **快速原型制作：** A2C
- **样品效率至关重要：** SAC、TD3 或 DQN
- **部分可观察性：** RecurrentPPO

## 训练配置提示

### 对于同策略算法（PPO、A2C）

<<<代码块_1>>>

### 对于离策略算法（SAC、TD3、DQN）

<<<代码块_2>>>

## 常见陷阱

1. **将 DQN 与连续操作一起使用** - DQN 仅适用于离散操作
2. **不使用带有 PPO/A2C 的矢量化环境** - 浪费潜在的加速
3. **使用的环境太少** - on-policy方法需要很多样本
4. **使用太大的重播缓冲区** - 可能导致内存问题
5. **不调整学习率** - 对于稳定训练至关重要
6. **忽略奖励缩放** - 标准化奖励以实现更好的学习
7. **错误的策略类型** - 对图像使用“CnnPolicy”，对字典观察使用“MultiInputPolicy”

## 性能基准

常见基准的近似预期绩效（平均奖励）：

### 连续控制 (MuJoCo)
- **HalfCheetah-v3**：PPO ~1800，SAC ~12000，TD3 ~9500
- **料斗-v3**：PPO ~2500，SAC ~3600，TD3 ~3600
- **Walker2d-v3**：PPO ~3000，SAC ~5500，TD3 ~5000

### 离散控制（雅达利）
- **突破**：PPO ~400，DQN ~300
- **Pong**：PPO ~20，DQN ~20
- **太空侵略者**：PPO ~1000，DQN ~800

*注意：性能随超参数和训练时间的不同而有很大差异。*

## 其他资源
- **RL Baselines3 Zoo**：预训练代理和超参数的集合：https://github.com/DLR-RM/rl-baselines3-zoo
- **超参数调优**：使用 Optuna 进行系统调优
- **自定义策略**：扩展自定义网络架构的基本策略
- **Contribution Repo**：用于实验算法的 SB3-Contrib（QR-DQN、TQC 等）